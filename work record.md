æºç ç†è§£
DLA ç±»çš„ __init__ å‡½æ•°è§£é‡Š
å‚æ•°è¯´æ˜Ž
- levels: ä¸€ä¸ªåˆ—è¡¨ï¼Œè¡¨ç¤ºæ¯ä¸ªå±‚çº§çš„å±‚æ•°ã€‚ä¾‹å¦‚ [1, 1, 1, 2, 2, 1] è¡¨ç¤ºæœ‰ 6 ä¸ªå±‚çº§ï¼Œæ¯ä¸ªå±‚çº§çš„å±‚æ•°åˆ†åˆ«ä¸º 1, 1, 1, 2, 2, 1ã€‚
- channels: ä¸€ä¸ªåˆ—è¡¨ï¼Œè¡¨ç¤ºæ¯ä¸ªå±‚çº§çš„é€šé“æ•°ã€‚ä¾‹å¦‚ [16, 32, 64, 128, 256, 512] è¡¨ç¤ºæœ‰ 6 ä¸ªå±‚çº§ï¼Œæ¯ä¸ªå±‚çº§çš„é€šé“æ•°åˆ†åˆ«ä¸º 16, 32, 64, 128, 256, 512ã€‚
- num_classes: æ•´æ•°ï¼Œè¡¨ç¤ºåˆ†ç±»ä»»åŠ¡çš„ç±»åˆ«æ•°ï¼Œé»˜è®¤ä¸º 1000ã€‚
- block: æ¨¡å—ç±»åž‹ï¼Œé»˜è®¤ä¸º BasicBlockã€‚å¯ä»¥æ˜¯ BasicBlock, Bottleneck, æˆ– BottleneckXã€‚
- residual_root: å¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦åœ¨æ ¹èŠ‚ç‚¹ä½¿ç”¨æ®‹å·®è¿žæŽ¥ï¼Œé»˜è®¤ä¸º Falseã€‚
- linear_root: å¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦åœ¨æ ¹èŠ‚ç‚¹ä½¿ç”¨çº¿æ€§å±‚ï¼Œé»˜è®¤ä¸º Falseã€‚
- opt: é…ç½®é€‰é¡¹ï¼Œé»˜è®¤ä¸º Noneã€‚

åˆå§‹åŒ–è¿‡ç¨‹
1. åˆå§‹åŒ–åŸºæœ¬å±žæ€§
self.channels = channels
self.num_classes = num_classes


2. å®šä¹‰åŸºç¡€å±‚
self.base_layer = nn.Sequential(
    nn.Conv2d(3, channels[0], kernel_size=7, stride=1, padding=3, bias=False),
    nn.BatchNorm2d(channels[0], momentum=BN_MOMENTUM),
    nn.ReLU(inplace=True)
)

- nn.Conv2d(3, channels[0], kernel_size=7, stride=1, padding=3, bias=False): è¾“å…¥é€šé“æ•°ä¸º 3ï¼ˆRGB å›¾åƒï¼‰ï¼Œè¾“å‡ºé€šé“æ•°ä¸º channels[0]ï¼Œå·ç§¯æ ¸å¤§å°ä¸º 7x7ï¼Œæ­¥é•¿ä¸º 1ï¼Œå¡«å……ä¸º 3ï¼Œä¸ä½¿ç”¨åç½®ã€‚
- nn.BatchNorm2d(channels[0], momentum=BN_MOMENTUM): æ‰¹å½’ä¸€åŒ–å±‚ï¼ŒåŠ¨é‡ä¸º BN_MOMENTUMã€‚
- nn.ReLU(inplace=True): ReLU æ¿€æ´»å‡½æ•°ï¼ŒåŽŸåœ°æ“ä½œã€‚

3. å®šä¹‰é¢„å¤„ç†å±‚ï¼ˆå¯é€‰ï¼‰
if opt.pre_img:
    self.pre_img_layer = nn.Sequential(
        nn.Conv2d(3, channels[0], kernel_size=7, stride=1, padding=3, bias=False),
        nn.BatchNorm2d(channels[0], momentum=BN_MOMENTUM),
        nn.ReLU(inplace=True)
    )
if opt.pre_hm:
    self.pre_hm_layer = nn.Sequential(
        nn.Conv2d(1, channels[0], kernel_size=7, stride=1, padding=3, bias=False),
        nn.BatchNorm2d(channels[0], momentum=BN_MOMENTUM),
        nn.ReLU(inplace=True)
    )

- pre_img_layer å’Œ pre_hm_layer åˆ†åˆ«ç”¨äºŽå¤„ç†å‰ä¸€å¸§çš„å›¾åƒå’Œçƒ­å›¾ï¼Œç»“æž„ä¸Ž base_layer ç›¸åŒã€‚

4. å®šä¹‰å±‚çº§
self.level0 = self._make_conv_level(channels[0], channels[0], levels[0])
self.level1 = self._make_conv_level(channels[0], channels[1], levels[1], stride=2)
self.level2 = Tree(levels[2], block, channels[1], channels[2], 2, level_root=False, root_residual=residual_root)
self.level3 = Tree(levels[3], block, channels[2], channels[3], 2, level_root=True, root_residual=residual_root)
self.level4 = Tree(levels[4], block, channels[3], channels[4], 2, level_root=True, root_residual=residual_root)
self.level5 = Tree(levels[5], block, channels[4], channels[5], 2, level_root=True, root_residual=residual_root)

- _make_conv_level æ–¹æ³•ç”¨äºŽåˆ›å»ºå·ç§¯å±‚ã€‚
- Tree ç±»ç”¨äºŽåˆ›å»ºæ ‘çŠ¶ç»“æž„çš„å±‚çº§ï¼Œæ¯ä¸ªå±‚çº§å¯ä»¥åŒ…å«å¤šä¸ªå­å±‚çº§ã€‚

5. å®šä¹‰èžåˆæ¨¡å—
self.fusion = Transformer_Fusion(d_model=16, nhead=4, num_fusion_encoder_layers=1, dim_feedforward=64)
self.fusion_m = Transformer_Fusion_M(d_model=16, nhead=4, num_fusion_encoder_layers=1, dim_feedforward=64)
self.pos_encoding = PositionEmbeddingSine(num_pos_feats=16//2, sine_type='lin_sine', avoid_aliazing=True, max_spatial_resolution=60)
self.patchembed = nn.Conv2d(16, 16, kernel_size=16, stride=16)

- Transformer_Fusion å’Œ Transformer_Fusion_M æ˜¯è‡ªå®šä¹‰çš„èžåˆæ¨¡å—ã€‚
- PositionEmbeddingSine ç”¨äºŽç”Ÿæˆä½ç½®ç¼–ç ã€‚
- patchembed æ˜¯ä¸€ä¸ªå·ç§¯å±‚ï¼Œç”¨äºŽå°†ç‰¹å¾å›¾è½¬æ¢ä¸º patchã€‚

6. è¾…åŠ©æ–¹æ³•
def _make_level(self, block, inplanes, planes, blocks, stride=1):
    downsample = None
    if stride != 1 or inplanes != planes:
        downsample = nn.Sequential(
            nn.MaxPool2d(stride, stride=stride),
            nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, bias=False),
            nn.BatchNorm2d(planes, momentum=BN_MOMENTUM),
        )

    layers = []
    layers.append(block(inplanes, planes, stride, downsample=downsample))
    for i in range(1, blocks):
        layers.append(block(inplanes, planes))

    return nn.Sequential(*layers)

def _make_conv_level(self, inplanes, planes, convs, stride=1, dilation=1):
    modules = []
    for i in range(convs):
        modules.extend([
            nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride if i == 0 else 1, padding=dilation, bias=False, dilation=dilation),
            nn.BatchNorm2d(planes, momentum=BN_MOMENTUM),
            nn.ReLU(inplace=True)
        ])
        inplanes = planes
    return nn.Sequential(*modules)

- _make_level æ–¹æ³•ç”¨äºŽåˆ›å»ºå¸¦æœ‰ä¸‹é‡‡æ ·çš„å±‚çº§ã€‚
- _make_conv_level æ–¹æ³•ç”¨äºŽåˆ›å»ºå¤šä¸ªå·ç§¯å±‚ã€‚

7. è¾…åŠ©æ–¹æ³•ï¼ˆç‰¹å¾è½¬æ¢ï¼‰
def token2feature(self, tokens, h, w):
    L, B, D = tokens.shape
    x = tokens.permute(1, 2, 0).view(B, D, H, W).contiguous()
    return x

def feature2token(self, x):
    B, C, W, H = x.shape
    L = W * H
    tokens = x.view(B, C, L).permute(2, 0, 1).contiguous()
    return tokens

def get_positional_encoding(self, feat):
    b, _, h, w = feat.shape
    mask = torch.zeros((b, h, w), dtype=torch.bool, device=feat.device)
    pos = self.pos_encoding(mask)
    return pos.reshape(b, -1, h, w)

- token2feature å°† token è½¬æ¢ä¸ºç‰¹å¾å›¾ã€‚
- feature2token å°†ç‰¹å¾å›¾è½¬æ¢ä¸º tokenã€‚
- get_positional_encoding ç”Ÿæˆä½ç½®ç¼–ç ã€‚

è¾…åŠ©æ–¹æ³•å‡½æ•°è§£é‡Š

1. token2feature(self, tokens, h, w)
è¿™ä¸ªå‡½æ•°çš„ä½œç”¨æ˜¯å°† token è½¬æ¢ä¸ºç‰¹å¾å›¾ã€‚
- å‚æ•°:
  - tokens: å½¢çŠ¶ä¸º (L, B, D) çš„ tensorï¼Œå…¶ä¸­ L æ˜¯ token æ•°é‡ï¼ŒB æ˜¯ batch å¤§å°ï¼ŒD æ˜¯æ¯ä¸ª token çš„ç»´åº¦ã€‚
  - h: ç‰¹å¾å›¾çš„é«˜åº¦ã€‚
  - w: ç‰¹å¾å›¾çš„å®½åº¦ã€‚
- æ­¥éª¤:
  1. èŽ·å– tokens çš„å½¢çŠ¶ (L, B, D)ã€‚
  2. è®¾ç½® H å’Œ W ä¸ºä¼ å…¥çš„é«˜åº¦å’Œå®½åº¦ã€‚
  3. ä½¿ç”¨ permute æ–¹æ³•å°† tokens çš„å½¢çŠ¶ä»Ž (L, B, D) è½¬æ¢ä¸º (B, D, L)ã€‚
  4. ä½¿ç”¨ view æ–¹æ³•å°†å½¢çŠ¶ä»Ž (B, D, L) è½¬æ¢ä¸º (B, D, H, W)ã€‚
  5. ä½¿ç”¨ contiguous æ–¹æ³•ç¡®ä¿ tensor åœ¨å†…å­˜ä¸­æ˜¯è¿žç»­çš„ï¼Œä»¥ä¾¿åŽç»­æ“ä½œã€‚
- è¿”å›ž:
  - å½¢çŠ¶ä¸º (B, D, H, W) çš„ç‰¹å¾å›¾ã€‚
def token2feature(self, tokens, h, w):
    L, B, D = tokens.shape
    H, W = h, w
    x = tokens.permute(1, 2, 0).view(B, D, H, W).contiguous()
    return x

2. feature2token(self, x)
è¿™ä¸ªå‡½æ•°çš„ä½œç”¨æ˜¯å°†ç‰¹å¾å›¾è½¬æ¢ä¸º tokenã€‚
- å‚æ•°:
  - x: å½¢çŠ¶ä¸º (B, C, W, H) çš„ç‰¹å¾å›¾ï¼Œå…¶ä¸­ B æ˜¯ batch å¤§å°ï¼ŒC æ˜¯é€šé“æ•°ï¼ŒW å’Œ H åˆ†åˆ«æ˜¯ç‰¹å¾å›¾çš„å®½åº¦å’Œé«˜åº¦ã€‚
- æ­¥éª¤:
  1. èŽ·å– x çš„å½¢çŠ¶ (B, C, W, H)ã€‚
  2. è®¡ç®— L ä¸º W * Hï¼Œå³ token çš„æ•°é‡ã€‚
  3. ä½¿ç”¨ view æ–¹æ³•å°†å½¢çŠ¶ä»Ž (B, C, W, H) è½¬æ¢ä¸º (B, C, L)ã€‚
  4. ä½¿ç”¨ permute æ–¹æ³•å°†å½¢çŠ¶ä»Ž (B, C, L) è½¬æ¢ä¸º (L, B, C)ã€‚
  5. ä½¿ç”¨ contiguous æ–¹æ³•ç¡®ä¿ tensor åœ¨å†…å­˜ä¸­æ˜¯è¿žç»­çš„ï¼Œä»¥ä¾¿åŽç»­æ“ä½œã€‚
- è¿”å›ž:
  - å½¢çŠ¶ä¸º (L, B, C) çš„ tokenã€‚
def feature2token(self, x):
    B, C, W, H = x.shape
    L = W * H
    tokens = x.view(B, C, L).permute(2, 0, 1).contiguous()
    return tokens

3. get_positional_encoding(self, feat)
è¿™ä¸ªå‡½æ•°çš„ä½œç”¨æ˜¯ç”Ÿæˆä½ç½®ç¼–ç ã€‚

- å‚æ•°:
  - feat: å½¢çŠ¶ä¸º (b, c, h, w) çš„ç‰¹å¾å›¾ï¼Œå…¶ä¸­ b æ˜¯ batch å¤§å°ï¼Œc æ˜¯é€šé“æ•°ï¼Œh å’Œ w åˆ†åˆ«æ˜¯ç‰¹å¾å›¾çš„é«˜åº¦å’Œå®½åº¦ã€‚
- æ­¥éª¤:
  1. èŽ·å– feat çš„å½¢çŠ¶ (b, c, h, w)ã€‚
  2. åˆ›å»ºä¸€ä¸ªå½¢çŠ¶ä¸º (b, h, w) çš„å…¨é›¶ maskï¼Œæ•°æ®ç±»åž‹ä¸ºå¸ƒå°”å€¼ï¼Œè®¾å¤‡ä¸Ž feat ç›¸åŒã€‚
  3. ä½¿ç”¨ self.pos_encoding ç”Ÿæˆä½ç½®ç¼–ç ï¼Œè¾“å…¥ä¸º maskã€‚
  4. ä½¿ç”¨ reshape æ–¹æ³•å°†ä½ç½®ç¼–ç çš„å½¢çŠ¶ä»Ž (b, -1, h, w) è½¬æ¢ä¸º (b, -1, h, w)ã€‚
- è¿”å›ž:
  - å½¢çŠ¶ä¸º (b, -1, h, w) çš„ä½ç½®ç¼–ç ã€‚
def get_positional_encoding(self, feat):
    b, _, h, w = feat.shape
    mask = torch.zeros((b, h, w), dtype=torch.bool, device=feat.device)
    pos = self.pos_encoding(mask)
    return pos.reshape(b, -1, h, w)

æ€»ç»“
token2feature å’Œ feature2token æ˜¯äº’é€†çš„æ“ä½œï¼Œåˆ†åˆ«ç”¨äºŽå°† token è½¬æ¢ä¸ºç‰¹å¾å›¾å’Œå°†ç‰¹å¾å›¾è½¬æ¢ä¸º tokenã€‚
get_positional_encoding ç”¨äºŽç”Ÿæˆä½ç½®ç¼–ç ï¼Œé€šå¸¸åœ¨ transformer æ¨¡åž‹ä¸­ç”¨äºŽæä¾›ä½ç½®ä¿¡æ¯ã€‚


DLA ç±»çš„ forward å‡½æ•°è§£é‡Š

è¿™ä¸ªå‡½æ•°å®šä¹‰äº†ä¸€ä¸ªæ¨¡åž‹çš„å‰å‘ä¼ æ’­è¿‡ç¨‹ã€‚å®ƒæŽ¥å—å¤šä¸ªè¾“å…¥ï¼Œå¹¶é€šè¿‡ä¸€ç³»åˆ—æ“ä½œç”Ÿæˆè¾“å‡ºã€‚ä»¥ä¸‹æ˜¯è¯¦ç»†çš„æ­¥éª¤è§£é‡Šï¼š

å‚æ•°
- x_rgb: RGB å›¾åƒçš„è¾“å…¥ç‰¹å¾å›¾ã€‚
- x_t: çƒ­çº¢å¤–å›¾åƒçš„è¾“å…¥ç‰¹å¾å›¾ã€‚
- pre_img_rgb: å‰ä¸€å¸§çš„ RGB å›¾åƒç‰¹å¾å›¾ï¼ˆå¯é€‰ï¼‰ã€‚
- pre_img_t: å‰ä¸€å¸§çš„çƒ­çº¢å¤–å›¾åƒç‰¹å¾å›¾ï¼ˆå¯é€‰ï¼‰ã€‚
- pre_hm: å‰ä¸€å¸§çš„çƒ­å›¾ï¼ˆå¯é€‰ï¼‰ã€‚

æ­¥éª¤
1. åˆå§‹åŒ–è¾“å‡ºåˆ—è¡¨:
y = []
2. åŸºç¡€å±‚å¤„ç†:
  - å¯¹ x_rgb å’Œ x_t è¿›è¡ŒåŸºç¡€å±‚å¤„ç†ã€‚
x_rgb = self.base_layer(x_rgb)
x_t = self.base_layer(x_t)
3. å‰ä¸€å¸§ç‰¹å¾å›¾å¤„ç†:
  - å¯¹å‰ä¸€å¸§çš„ç‰¹å¾å›¾è¿›è¡Œé¢„å¤„ç†ã€‚
pre_img_rgb = self.pre_img_layer(pre_img_rgb)
pre_img_t = self.pre_img_layer(pre_img_t)
pre_hm = self.pre_hm_layer(pre_hm)
4. Patch Embedding:
  - å°†ç‰¹å¾å›¾è½¬æ¢ä¸º patch embeddingã€‚
x_rgb_p = self.patchembed(x_rgb)
x_t_p = self.patchembed(x_t)
pre_x_rgb_p = self.patchembed(pre_img_rgb)
pre_x_t_p = self.patchembed(pre_img_t)
pre_hm_p = self.patchembed(pre_hm)
5. ç‰¹å¾å›¾è½¬ token:
  - å°† patch embedding è½¬æ¢ä¸º tokenã€‚
x_rgb_token = self.feature2token(x_rgb_p)
x_t_token = self.feature2token(x_t_p)
pre_x_rgb_token = self.feature2token(pre_x_rgb_p)
pre_x_t_token = self.feature2token(pre_x_t_p)
pre_hm_token = self.feature2token(pre_hm_p)
6. ä½ç½®ç¼–ç :
  - èŽ·å–ä½ç½®ç¼–ç å¹¶å°†å…¶è½¬æ¢ä¸º tokenã€‚
pos_embed = self.get_positional_encoding(x_rgb_p)
pos_embed_token = self.feature2token(pos_embed)
7. èžåˆæ“ä½œ:
  - ä½¿ç”¨ fusion_m æ–¹æ³•å¯¹å½“å‰å¸§å’Œå‰ä¸€å¸§çš„ token è¿›è¡Œèžåˆã€‚
x_rgb_all = self.fusion_m(x_rgb_token, pre_x_rgb_token, pre_hm_token, pos_embed_token)
x_t_all = self.fusion_m(x_t_token, pre_x_t_token, pre_hm_token, pos_embed_token)
8. æœ€ç»ˆèžåˆ:
  - ä½¿ç”¨ fusion æ–¹æ³•å¯¹èžåˆåŽçš„ token è¿›è¡Œæœ€ç»ˆèžåˆã€‚
[B, C, H, W] = x_rgb.shape
result = self.fusion(x_rgb_all, x_t_all, pos_embed_token)
9. Token è½¬ç‰¹å¾å›¾:
  - å°†èžåˆç»“æžœè½¬æ¢å›žç‰¹å¾å›¾ã€‚
[b, c, h, w] = x_rgb_p.shape
x_p = self.token2feature(result, h, w)
10. ä¸Šé‡‡æ ·å’Œæ®‹å·®è¿žæŽ¥:
  - å¯¹ç‰¹å¾å›¾è¿›è¡Œä¸Šé‡‡æ ·ï¼Œå¹¶ä¸ŽåŽŸå§‹è¾“å…¥ç‰¹å¾å›¾è¿›è¡Œæ®‹å·®è¿žæŽ¥ã€‚
x = F.interpolate(x_p, size=(H, W), mode='bilinear', align_corners=True) + x_rgb + x_t
11. å¤šå°ºåº¦ç‰¹å¾æå–:
  - é€šè¿‡å¤šä¸ªå±‚çº§çš„ç‰¹å¾æå–æ¨¡å—ï¼Œç”Ÿæˆå¤šå°ºåº¦ç‰¹å¾å›¾ã€‚
for i in range(6):
    x = getattr(self, 'level{}'.format(i))(x)
    y.append(x)
12. è¿”å›žç»“æžœ:
  - è¿”å›žå¤šå°ºåº¦ç‰¹å¾å›¾åˆ—è¡¨ã€‚
return y

æ€»ç»“
- è¾“å…¥: RGB å›¾åƒç‰¹å¾å›¾ã€çƒ­çº¢å¤–å›¾åƒç‰¹å¾å›¾ã€å‰ä¸€å¸§çš„ RGB å’Œçƒ­çº¢å¤–å›¾åƒç‰¹å¾å›¾ã€å‰ä¸€å¸§çš„çƒ­å›¾ã€‚
- å¤„ç†: é€šè¿‡åŸºç¡€å±‚ã€å‰ä¸€å¸§ç‰¹å¾å›¾å¤„ç†ã€patch embeddingã€ç‰¹å¾å›¾è½¬ tokenã€ä½ç½®ç¼–ç ã€èžåˆæ“ä½œã€æœ€ç»ˆèžåˆã€Token è½¬ç‰¹å¾å›¾ã€ä¸Šé‡‡æ ·å’Œæ®‹å·®è¿žæŽ¥ã€å¤šå°ºåº¦ç‰¹å¾æå–ç­‰æ­¥éª¤ã€‚
- è¾“å‡º: å¤šå°ºåº¦ç‰¹å¾å›¾åˆ—è¡¨ã€‚

ç»´åº¦å˜æ¢

For prime image (if set C = 16)
  initial shape: [B, 3, H, W] === base_layer ===> [B, 16, H, W]
  [B, 16, H, W] === patchembed ===> [B, 16, H//16, W//16]
  [B, 16, H//16, W//16] === feature2token ===> [(H//16)*(W//16), B, 16]
  ( 
  [B, 16, H//16, W//16] === get_positional_encoding ===> [B, 16, H//16, W//16]
  [B, 16, H//16, W//16] === feature2token ===> [(H//16)*(W//16), B, 16]
   )
  [(H//16)*(W//16), B, 16] === fusion_m ===>  [(H//16)*(W//16), B, 16]
  [(H//16)*(W//16), B, 16] === token2feature ===> [B, 16, H//16, W//16]
  [B, 16, H//16, W//16] === F.interpolate ===> [B, 16, H, W]

For the previous image, same as primes

For the heatmap  (if set C = 16)
initial shape: [B, 1, H, W] === base_layer ===> [B, 16, H, W]
others same as primes

å‡½æ•°æ¢³ç†
def feature2token(self, x)
    B, C, W, H = x.shape    # batch size, channel, height, width
    L = W * H    # patches
    tokens = x.view(B, C, L).permute(2, 0, 1).contiguous()
        # view : reshape x[B,C,W,H] as x[B,C,L], 
        #        which combine height and width into one dimension.
        # permute: rearrange x[B,C,L] as X[L,B,C]
        # contiguous: make sure Tensor is contiguous in memory
        
def token2feature(self,tokens,h,w):
    L,B,D=tokens.shape    # patches, batch size, feature dimention(Channel)
    H,W=h,w
    x = tokens.permute(1, 2, 0).view(B, D, H, W).contiguous()
        # permute: rearrange tokens[L,B,D] as tokens[B,D,L]
        # view: reshape last dimension into 2 dim{Height, Width}
        # contiguous: make sure Tensor is contiguous in memory
    return x
fusion_m æ¢³ç†
fusion_m call:
  d_model = 16
  nhead = 4
  num_fusion_encoder_layers  1
  dim_feedforward = 64
call fusion_m(img, pre, hm, pos_embed)

---
Transformer_Fusion_M
  d_model = 16
  nhead = 4
  encoder_layer = TransformerEncoderLayer(d_model=16, nhead=4, dim_feedforward=64, dropout=0.1, activation=â€reluâ€, normalize_before=False)
  encoder = TransformerEncoder(encoder_layer, num_fusion_encoder_layers =1, encoder_norm = None)

---
Transformer_Fusion_M. Forward()           call encoder(img, pre, hm, pos_embed)
TransformerEncoder
  layers = _get_clones(encoder_layer, num_layers = 1) = nn.ModuleList(TransformerEncoderLayer)
  output = layers
TransformerEncoderLayer
  d_model = 16
  nhead = 4
  dim_feedforward = 64
  dropout=0.1
  activation=â€reluâ€
  
  multihead_attn = nn.MultiheadAttention(16, 4, dropout=0.1)
  linear1 = nn.Linear(16, 64)
  linear2 = nn.Linear(64, 16)
  dropout = dropout1 = dropout2 = nn.Dropout(0.1)
  norm1 = norm2 = nn.LayerNorm(16)
  activation = F.relu

as call encoder(img, pre, hm, pos_embed)
  src = img
  pre_src = pre
  pre_hm = hm
  pos = pos_embed

get_positional_encoding æ¢³ç†
mask   torch.zeros(( b, h, w))        # all-zero

---
PositionEmbeddingSine
  num_pos_feats = 8
  sine_type = â€˜lin_sineâ€™
  avoid_aliazing = True
  max_spatial_resolution = 60
  temperature = 10000
  sine = NerfPositionalEncoding(depth = 4, sine_type = â€˜lin_sineâ€™, avoid_aliazing = True, max_spatial_resolution = 60)
  
  ~mask     # all-one
  â€¦â€¦
  pos   torch.stack([x_embed, y_embed], dim=-1)     dimension: [b, h, w, 2]
  out = self.sine(pos).permute(0, 3, 1, 2)

---
call sine(pos)
sine(pos) return a Tensor of (b, h, w, 16), which will be permuted as (b, 16, h, w) 

---
NerfPosifionalEncoding
  depth = 4
  sine_type = â€˜lin_sineâ€™
  avoid_aliazing = True
  max_spatial_resolution = 60
  bases = [1, 2, 3, 4]
  factor = 60/4 = 15

  out = torch.cat([torch.sin(i * self.factor * math.pi * inputs) for i in self.bases] +
                          [torch.cos(i * self.factor * math.pi * inputs) for i in self.bases], axis=-1)
          # [b, h, w, 2*depth*2 = 16]


YOLO ä»£ç 
æ•°æ®å¤„ç†
ç”±äºŽYOLOå®žæ—¶æ£€æµ‹çš„ç‰¹ç‚¹ï¼Œé€‚åˆå¯¹å•å¼ å›¾åƒè¿›è¡Œå¤„ç†ã€‚çº¢å¤–å›¾åƒçš„æ—¶åºä¿¡æ¯å°è¯•é€šè¿‡ç»´åº¦å †å çš„æ–¹å¼èžåˆåˆ°å•å¼ å›¾åƒä¸­ã€‚
å‡è®¾çº¢å¤–å›¾åƒimg : [1, h, w] , å‰å¸§å›¾åƒ pre_img : [1, h, w]ï¼Œé€šè¿‡pre_img ç”Ÿæˆå‰å¸§çƒ­å›¾ pre_hm : [1, h, w]
åˆæˆå›¾åƒ x : [3, h, w] ï¼Œå †å æ–¹å¼ 3 ==> {img, pre_img, pre_hm}

å•å¹…åˆæˆå›¾åƒä¸­åŒ…å«æ—¶åºä¿¡æ¯ï¼Œä¸éœ€è‡ªå®šä¹‰ Dataset ç±»ã€‚è¿è¡Œå•ç‹¬çš„è„šæœ¬æå‰å¤„ç†æ•°æ®é›†ã€‚
æµ‹è¯•éƒ¨åˆ†è¿˜æ²¡æƒ³å¥½
æ¨¡å—ç¼–å†™
åœ¨æºç DLAç±»åŸºç¡€ä¸Šä¿®æ”¹ï¼Œç¼–å†™InfraredHeatmapFusionç±»ï¼Œè®¡åˆ’ä½œä¸ºç½‘ç»œä¸»å¹²çš„è¾“å…¥å±‚ã€‚
æ¨¡å— InfraredHeatmapFusion , å¯¼å…¥åŒç›®å½•ä¸‹çš„ç±» 
  from .transformer_fusion import Transformer_Fusion_M
  from .position_encoding import PositionEmbeddingSine

ir_hm_fusion.py
ä»ŽPFTracké¡¹ç›®ä»£ç ä¸­æ¢³ç†å‡ºæ¥ä»Žè¾“å…¥å›¾åƒåˆ°èžåˆç»“æžœçš„å˜æ¢è¿‡ç¨‹ï¼Œè¾“å…¥3é€šé“åˆæˆå›¾ï¼Œæ‹†åˆ†æˆä¸‰ä¸ªå•é€šé“å›¾åƒåˆ†åˆ«å¤„ç†åŽè¿›è¡Œç‰¹å¾èžåˆã€‚
import torch
from torch import nn
import torch.nn.functional as F
import numpy as np

from .transformer_fusion import Transformer_Fusion_M
from .position_encoding import PositionEmbeddingSine

BN_MOMENTUM = 0.1

class InfraredHeatmapFusion(nn.Module):
    def __init__(self, c1, c2=16):
        super(InfraredHeatmapFusion, self).__init__()
        self.channel = c2
        self.base_layer = nn.Sequential(
            nn.Conv2d(1, c2, kernel_size=7, stride=1, padding=3, bias=False),
            nn.BatchNorm2d(c2, momentum=BN_MOMENTUM),
            nn.ReLU(inplace=True),
        )
        self.pre_img_layer = nn.Sequential(
            nn.Conv2d(1, c2, kernel_size=7, stride=1, padding=3, bias=False),
            nn.BatchNorm2d(c2, momentum=BN_MOMENTUM),
            nn.ReLU(inplace=True),
        )
        self.pre_hm_layer = nn.Sequential(
            nn.Conv2d(1, c2, kernel_size=7, stride=1, padding=3, bias=False),
            nn.BatchNorm2d(c2, momentum=BN_MOMENTUM),
            nn.ReLU(inplace=True),
        )
        self.fusion = Transformer_Fusion_M(d_model=16, nhead=4, num_fusion_encoder_layers=1, dim_feedforward=64)
        self.pos_encoding = PositionEmbeddingSine(num_pos_feats=16//2, sine_type='lin_sine', avoid_aliazing=True, max_spatial_resolution=60)
        self.patchembed = nn.Conv2d(16, 16, kernel_size=16, stride=16)


    def forward(self, x):
        img, pre_img, pre_hm = torch.split(x, 1, dim=1)
        img = self.base_layer(img)

        pre_img = self.pre_img_layer(pre_img)
        pre_hm = self.pre_hm_layer(pre_hm)

        img_p = self.patchembed(img)
        pre_img_p = self.patchembed(pre_img)
        pre_hm_p = self.patchembed(pre_hm)

        img_token = self.feature2token(img_p)
        pre_img_token = self.feature2token(pre_img_p)
        pre_hm_token = self.feature2token(pre_hm_p)
        pos_embed = self.get_positional_encoding(img_p)
        pos_embed_token = self.feature2token(pos_embed)

        x_f = self.fusion(img_token, pre_img_token, pre_hm_token, pos_embed_token)
        [B,C,H,W] = img.shape
        [b,c,h,w] = img_p.shape
        x_p = self.token2feature(x_f, h, w)
        out = F.interpolate(x_p, size=(H, W), mode='bilinear', align_corners=True) + img
        return out

    def feature2token(self, x):
        B,C,W,H = x.shape
        L = W*H
        token = x.view(B,C,L).permute(2, 0, 1).contiguous()
        return token


    def token2feature(self, tokens, h, w):
        L,B,D = tokens.shape
        H,W = h,w
        x = tokens.permute(1, 2, 0).view(B,D,H,W).contiguous()
        return x
    

    def get_positional_encoding(self, feat):
        b, _, h, w = feat.shape
        mask = torch.zeros((b, h, w), dtype=torch.bool, device=feat.device)
        pos = self.pos_encoding(mask)
        return pos.reshape(b, -1, h, w)
    
position_encoding.py
è¿™é‡Œè‡ªå®šä¹‰m_cumsumå‡½æ•°æ¥æ›¿æ¢æºç çš„torch.cumsumè°ƒç”¨ã€‚åŽŸå› æ˜¯torch.cumsumè°ƒç”¨æ–¹å¼ä¸Žcudaå†…æ ¸ç›¸å…³ï¼Œä¼šäº§ç”Ÿä¸‹é¢çš„è­¦å‘Šï¼Œé‡‡ç”¨è‡ªå®šä¹‰å‡½æ•°æ˜Žç¡®åŒ–cumsumè®¡ç®—ï¼Œè¿ç®—é€Ÿåº¦æœ‰å°‘è®¸ä¸‹é™ã€‚
  /home/rody/code/ultralytics/ultralytics/nn/modules/ir/position_encoding.py:52:  UserWarning: cumsum_cuda_kernel does not have a deterministic  implementation, but you set 'torch.use_deterministic_algorithms(True,  warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues   to help us prioritize adding deterministic support for this operation.  (Triggered internally at  /opt/conda/conda-bld/pytorch_1720538438750/work/aten/src/ATen/Context.cpp:83.)   x_embed = not_mask.cumsum(2, dtype=torch.float32)

import math

import numpy as np
import torch
from numpy import dtype
from torch import nn

class NerfPositionalEncoding(nn.Module):
    def __init__(self, depth=10, sine_type='lin_sine', avoid_aliasing=False, max_spatial_resolution=None):
        '''
        out_dim = in_dim * depth * 2
        '''
        super().__init__()
        if sine_type == 'lin_sine':
            self.bases = [i+1 for i in range(depth)]
        elif sine_type == 'exp_sine':
            self.bases = [2**i for i in range(depth)]
        print(f'using {sine_type} as positional encoding')

        if avoid_aliasing and (max_spatial_resolution is None):
            raise ValueError('Please specify the maxima spatial resolution (h, w) of the feature map')
        elif avoid_aliasing:
            self.factor = max_spatial_resolution/depth
        else:
            self.factor = 1.

    @torch.no_grad()
    def forward(self, inputs):
        out = torch.cat([torch.sin(i * self.factor * math.pi * inputs) for i in self.bases] +
                        [torch.cos(i * self.factor * math.pi * inputs) for i in self.bases], axis=-1)
        #pdb.set_trace()
        assert torch.isnan(out).any() == False
        return out


def m_cumsum(input_tensor, dim, dtype=torch.float16):

    # print(input_tensor.dim())
    if input_tensor.dim() != 3:
        return torch.cumsum(input_tensor, dim)

    input_tensor = input_tensor.clone().detach().requires_grad_(False)
    result = torch.zeros_like(input_tensor, dtype=dtype)
    # print(result)
    if dim == 1:
        for i in range(input_tensor.shape[dim]):
            if i == 0:
                result[:, i, :] = input_tensor[:, i, :]
            else:
                result[:, i, :] = result[:, i - 1, :] + input_tensor[:, i, :]
    elif dim == 2:
        for i in range(input_tensor.shape[dim]):
            if i == 0:
                result[:, :, i] = input_tensor[:, :, i]
            else:
                result[:, :, i] = result[:, :, i - 1] + input_tensor[:, :, i]
    return result


class PositionEmbeddingSine(nn.Module):
    """
    This is a more standard version of the position embedding, very similar to the one
    used by the Attention is all you need paper, generalized to work on images.
    """
    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None, sine_type='lin_sine',
                 avoid_aliazing=False, max_spatial_resolution=None):
        super().__init__()
        self.num_pos_feats = num_pos_feats
        self.temperature = temperature
        self.normalize = normalize
        self.sine = NerfPositionalEncoding(num_pos_feats//2, sine_type, avoid_aliazing, max_spatial_resolution)

    @torch.no_grad()
    def forward(self, mask):

        assert mask is not None
        not_mask = ~mask
        # y_embed = not_mask.cumsum(1, dtype=torch.float32)
        # x_embed = not_mask.cumsum(2, dtype=torch.float32)
        y_embed = m_cumsum(not_mask, 1, dtype=torch.float16)
        x_embed = m_cumsum(not_mask, 2, dtype=torch.float16)
        # print(y_embed.dtype, x_embed.dtype)
        eps = 1e-6
        y_embed = (y_embed-0.5) / (y_embed[:, -1:, :] + eps)
        x_embed = (x_embed-0.5) / (x_embed[:, :, -1:] + eps)
        pos = torch.stack([x_embed, y_embed], dim=-1)
        out=self.sine(pos).permute(0, 3, 1, 2)
        return out
transformer_fusion.py 
yolov8 é»˜è®¤å¯ç”¨ AWPï¼ˆAutomatic Mixed Precisionï¼‰ï¼ŒTransformerEncoderLayer.forwardå…¥å‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¢« amp è‡ªåŠ¨è½¬æ¢ä¸ºtorch.float16ï¼Œä¸ºä¿æŒå¼ é‡æ­£å¸¸åŒ¹é…è®¡ç®—ï¼ŒæŠŠæ¶‰åŠå¤šå¤´æ³¨æ„åŠ›å±‚è®¡ç®—çš„å¼ é‡éƒ½è¿›è¡Œæ‰‹åŠ¨è½¬æ¢ã€‚å¦å¤–åœ¨è®­ç»ƒå‰é¡»æŒ‡å®šå‚æ•°dtype ä¸º torch.float32ã€‚
# --------------------------------------------------------------------*/
# This file includes code from https://github.com/facebookresearch/detr/blob/main/models/detr.py
# --------------------------------------------------------------------*/
#


import copy
import torch
import torch.nn.functional as F
from torch import nn
import pdb


class Transformer_Fusion_M(nn.Module):
    def __init__(self, d_model=64, nhead=8, num_fusion_encoder_layers=2, dim_feedforward=2048,
                 dropout=0.1, activation="relu", normalize_before=False, return_intermediate_dec=False):
        super().__init__()

        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation, normalize_before)
        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None
        self.encoder = TransformerEncoder(encoder_layer, num_fusion_encoder_layers, encoder_norm)

        self._reset_parameters()

        self.d_model = d_model
        self.nhead = nhead       
    def _reset_parameters(self):
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
    def with_pos_embed(self, tensor, pos):
        return tensor if pos is None else tensor + pos
    def forward(self, src, pre_src,pre_hm, pos_embed):

        output = self.encoder(src, pre_src, pre_hm, pos=pos_embed)
        return output


class TransformerEncoder(nn.Module):
    def __init__(self, encoder_layer, num_layers, norm=None):
        super().__init__()
        self.layers = _get_clones(encoder_layer, num_layers)
        self.norm = norm
    def forward(self, src, pre_src, pre_hm, pos=None):
        output = src
        pre_output = pre_src
        for layer in self.layers:
            output= layer(output, pre_output, pre_hm, pos=pos)
        return output

        
class TransformerEncoderLayer(nn.Module):

    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation="relu", normalize_before=False):
        super().__init__()    
        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        # Implementation of Feedforward model
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(dropout)        
        self.linear2 = nn.Linear(dim_feedforward, d_model)
        
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
         
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

        self.activation = _get_activation_fn(activation)
        self.normalize_before = normalize_before

    def with_pos_embed(self, tensor, pos):
        return tensor if pos is None else tensor + pos

    def forward(self, src, pre_src, pre_hm, pos=None, c=1, n=2):
        q = self.with_pos_embed(src, pos)
        k = self.with_pos_embed(pre_src, pos)
        if pos is not None: pos.to(dtype=torch.float16)
        # print("src: {}, pre_src: {}, pre_hm: {}, q: {}, k: {}, v: {}, pos: {}".format(src.dtype, pre_src.dtype, pre_hm.dtype, q.dtype, k.dtype, pre_src.dtype, pos.dtype))
        src1 = self.multihead_attn(q, k, value=pre_src)[0]
        src = src + self.dropout(src1)
        src = self.norm1(src)
        src = src + pre_hm
        src2 = self.linear2(self.dropout1(self.activation(self.linear1(src))))
        src = src + self.dropout2(src2)
        src = self.norm2(src)
        return src



def _get_clones(module, N):
    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])


def build_transformer_fusion(args):
    return Transformer_Fusion_M(
        d_model=args.hidden_dim,
        dropout=args.dropout,
        nhead=args.nheads,
        dim_feedforward=args.dim_feedforward,
        num_encoder_layers=args.enc_layers,
        normalize_before=args.pre_norm,
        return_intermediate_dec=True,
    )


def _get_activation_fn(activation):
    """Return an activation function given a string"""
    if activation == "relu":
        return F.relu
    if activation == "gelu":
        return F.gelu
    if activation == "glu":
        return F.glu
    raise RuntimeError(F"activation should be relu/gelu/glu, not {activation}.")
ultralytics/nn/tasks.py ä¿®æ”¹ parse_model(c, ch, verbose=True)
å¯¼å…¥iråŒ…ï¼Œæ·»åŠ å¯¹ InfraredHeatmapFusion æ¨¡å—çš„è§£æžã€‚
from ultralytics.nn.modules.ir.ir_hm_fusion import InfraredHeatmapFusion as IrHmFusion

def parse_model(d, ch, verbose=True):  # model_dict, input_channels(3)
    """Parse a YOLO model.yaml dictionary into a PyTorch model."""
    import ast

    # Args
    legacy = True  # backward compatibility for v3/v5/v8/v9 models
    max_channels = float("inf")
    nc, act, scales = (d.get(x) for x in ("nc", "activation", "scales"))
    depth, width, kpt_shape = (d.get(x, 1.0) for x in ("depth_multiple", "width_multiple", "kpt_shape"))
    if scales:
        scale = d.get("scale")
        if not scale:
            scale = tuple(scales.keys())[0]
            LOGGER.warning(f"WARNING âš ï¸ no model scale passed. Assuming scale='{scale}'.")
        depth, width, max_channels = scales[scale]

    if act:
        Conv.default_act = eval(act)  # redefine default activation, i.e. Conv.default_act = nn.SiLU()
        if verbose:
            LOGGER.info(f"{colorstr('activation:')} {act}")  # print

    if verbose:
        LOGGER.info(f"\n{'':>3}{'from':>20}{'n':>3}{'params':>10}  {'module':<45}{'arguments':<30}")
    ch = [ch]
    layers, save, c2 = [], [], ch[-1]  # layers, savelist, ch out
    for i, (f, n, m, args) in enumerate(d["backbone"] + d["head"]):  # from, number, module, args
        m = getattr(torch.nn, m[3:]) if "nn." in m else globals()[m]  # get module
        for j, a in enumerate(args):
            if isinstance(a, str):
                with contextlib.suppress(ValueError):
                    args[j] = locals()[a] if a in locals() else ast.literal_eval(a)
        n = n_ = max(round(n * depth), 1) if n > 1 else n  # depth gain
        if m in {
            Classify,Conv,ConvTranspose,GhostConv,Bottleneck,GhostBottleneck,SPP,SPPF,C2fPSA,C2PSA,DWConv,Focus,BottleneckCSP,C1,C2,C2f,C3k2,RepNCSPELAN4,ELAN1,ADown,AConv,SPPELAN,C2fAttn,C3,C3TR,C3Ghost,nn.ConvTranspose2d,DWConvTranspose2d,C3x,RepC3,PSA,SCDown,C2fCIB,
        }:
            c1, c2 = ch[f], args[0]
            if c2 != nc:  # if c2 not equal to number of classes (i.e. for Classify() output)
                c2 = make_divisible(min(c2, max_channels) * width, 8)
            if m is C2fAttn:
                args[1] = make_divisible(min(args[1], max_channels // 2) * width, 8)  # embed channels
                args[2] = int(
                    max(round(min(args[2], max_channels // 2 // 32)) * width, 1) if args[2] > 1 else args[2]
                )  # num heads

            args = [c1, c2, *args[1:]]
            if m in {
                BottleneckCSP,C1,C2,C2f,C3k2,C2fAttn,C3,C3TR,C3Ghost,C3x,RepC3,C2fPSA,C2fCIB,C2PSA,
            }:
                args.insert(2, n)  # number of repeats
                n = 1
            if m is C3k2:  # for M/L/X sizes
                legacy = False
                if scale in "mlx":
                    args[3] = True
        elif m is AIFI:
            args = [ch[f], *args]
        elif m in {HGStem, HGBlock}:
            c1, cm, c2 = ch[f], args[0], args[1]
            args = [c1, cm, c2, *args[2:]]
            if m is HGBlock:
                args.insert(4, n)  # number of repeats
                n = 1
        elif m is ResNetLayer:
            c2 = args[1] if args[3] else args[1] * 4
        elif m is nn.BatchNorm2d:
            args = [ch[f]]
        elif m is Concat:
            c2 = sum(ch[x] for x in f)
        elif m in {Detect, WorldDetect, Segment, Pose, OBB, ImagePoolingAttn, v10Detect}:
            args.append([ch[x] for x in f])
            if m is Segment:
                args[2] = make_divisible(min(args[2], max_channels) * width, 8)
            if m in {Detect, Segment, Pose, OBB}:
                m.legacy = legacy
        elif m is RTDETRDecoder:  # special case, channels arg must be passed in index 1
            args.insert(1, [ch[x] for x in f])
        elif m is CBLinear:
            c2 = args[0]
            c1 = ch[f]
            args = [c1, c2, *args[1:]]
        elif m is CBFuse:
            c2 = ch[f[-1]]
        elif m is IrHmFusion:
            c2 = args[0]
            args = [c2, *args]
        else:
            c2 = ch[f]

        m_ = nn.Sequential(*(m(*args) for _ in range(n))) if n > 1 else m(*args)  # module
        t = str(m)[8:-2].replace("__main__.", "")  # module type
        m_.np = sum(x.numel() for x in m_.parameters())  # number params
        m_.i, m_.f, m_.type = i, f, t  # attach index, 'from' index, type
        if verbose:
            LOGGER.info(f"{i:>3}{str(f):>20}{n_:>3}{m_.np:10.0f}  {t:<45}{str(args):<30}")  # print
        save.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1)  # append to savelist
        layers.append(m_)
        if i == 0:
            ch = []
        ch.append(c2)
    return nn.Sequential(*layers), sorted(save)


è®­ç»ƒæµ‹è¯•
yamlæ–‡ä»¶ 
æŠŠç¬¬0å±‚æ¢æˆ InfraredHeatmapFusion æ¨¡å—ã€‚
æ¨¡åž‹ç»“æž„
                   from  n    params  module                                       arguments                     
using lin_sine as positional encoding
  0                  -1  1     71280  ultralytics.nn.modules.ir.ir_hm_fusion.InfraredHeatmapFusion[16, 16]                      
  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                
  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             
  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                
  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             
  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               
  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           
  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              
  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           
  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 
 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          
 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           
 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 
 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          
 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           
 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  
 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                
 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           
 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 
 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              
 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           
 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 
 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           
YOLOv8-irhm summary: 251 layers, 3,082,054 parameters, 3,082,038 gradients



åœ¨æœ¬æœºä¸Šè®­ç»ƒï¼š
model.to(dtype=torch.float32, device='cuda')
result = model.train(data='vtuav-irhm.yaml',
                     name='yolov8_irhm',
                     epochs=50,
                     workers=8,
                     batch=1)

ç»“æžœï¼š
tm
Validating runs/detect/yolov8_irhm6/weights/best.pt...
WARNING âš ï¸ validating an untrained model YAML will result in 0 mAP.
Ultralytics 8.3.35 ðŸš€ Python-3.8.20 torch-2.4.0 CUDA:0 (NVIDIA GeForce GTX 1650 Ti, 3897MiB)
YOLOv8-irhm summary (fused): 195 layers, 3,076,870 parameters, 0 gradients
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:05<00:00,  3.12it/s]
                   all         67        915     0.0128      0.144      0.008    0.00161
                person         67        612    0.00422    0.00327    0.00212   0.000212
                   car         42        303     0.0215      0.284     0.0139      0.003
Speed: 0.3ms preprocess, 78.1ms inference, 0.0ms loss, 0.7ms postprocess per image
Results saved to runs/detect/yolov8_irhm6


Datasetç±»

12.02 ç»§æ‰¿torch.util.data.Dataset çš„è‡ªå®šä¹‰Datasetç±»å®žçŽ°ï¼Œæœªæµ‹è¯•ã€‚å‡†å¤‡å†ä¿®æ”¹ï¼Œåˆ°YOLOä¸­ä½¿ç”¨ã€‚

ç»§æ‰¿YOLODatasetç±»ï¼Œé‡å†™__getitem__ã€‚é‡å†™BaseDatasetä¸­çš„ load_imageï¼Œåœ¨è¿™ä¸€çº§å®Œæˆ image concatã€‚
  
  image concat å¿…éœ€å…¥å‚ï¼š idxï¼Œ ann_path 
  img2label_paths æŠŠå›¾åƒæ–‡ä»¶è·¯å¾„è½¬æ¢ä¸ºå¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶è·¯å¾„ ã€‚ä»¿å†™æ­¤å‡½æ•°ã€‚
  
  img_path = '/home/rody/datas/irhm1128/train'
  im_files æ˜¯str list , å‚¨å­˜è·¯å¾„ å¦‚ ['/home/rody/datas/irhm1128/train/images/000001.jpg', ... ]
  label_files æ˜¯str list , å‚¨å­˜è·¯å¾„ å¦‚ ['/home/rody/datas/irhm1128/train/images/000001.txt', ... ]
  
  self.label_files[i]


ultralytics/data/dataset.py
  è‡ªå®šä¹‰ç±»YOLOIRHMDataset , ç»§æ‰¿ YOLODataset, é‡å†™ load_image, å®Œæˆhm generate ä¸Ž image concat.
import math

class YOLOIRHMDataset(YOLODataset):
    def __init__(self, *args, data=None, task="detect", **kwargs):
        super().__init__( *args, data=data, task=task, **kwargs)

    def _load_ann(self, filename):
        with open(filename, 'r') as anns_file:
            return [ann.strip() for ann in anns_file.readlines()]
    def load_image(self, i, rect_mode=True):
        """Loads 1 image from dataset index 'i', returns (im, resized hw)."""
        im, f, fn = self.ims[i], self.im_files[i], self.npy_files[i]
        if im is None:  # not cached in RAM
            if fn.exists():  # load npy
                try:
                    im = np.load(fn)
                except Exception as e:
                    LOGGER.warning(f"{self.prefix}WARNING âš ï¸ Removing corrupt *.npy image file {fn} due to: {e}")
                    Path(fn).unlink(missing_ok=True)
                    im = cv2.imread(f, cv2.IMREAD_GRAYSCALE)
            else:  # read image
                im = cv2.imread(f, cv2.IMREAD_GRAYSCALE)
            if im is None:
                raise FileNotFoundError(f"Image Not Found {f}")

            if i > 0:
                prev_i = i - 1
                prev_im = cv2.imread(self.im_files[prev_i], cv2.IMREAD_GRAYSCALE)
                prev_ann = self.label_files[i - 1]
            else:
                prev_im = im
                prev_ann = self.label_files[i]
            # check similarity
            similarity = self.compare_histograms(im, prev_im)
            if similarity < 0.9:
                prev_im, prev_ann = im, self.label_files[i]

            prev_hm, _ = self.generate_heatmap(prev_im, self._load_ann(prev_ann))

            # im = torch.from_numpy(im).permute(2, 0, 1).float()
            # prev_im = torch.from_numpy(prev_im).permute(2, 0, 1).float()
            # prev_hm = torch.from_numpy(prev_hm).float()

            assert im.shape == prev_im.shape == prev_hm.shape
            im = np.stack([im, prev_im, prev_hm], axis=-1)

            # im = torch.cat([im, prev_im, prev_hm], dim=0)

            h0, w0 = im.shape[:2]  # orig hw
            if rect_mode:  # resize long side to imgsz while maintaining aspect ratio
                r = self.imgsz / max(h0, w0)  # ratio
                if r != 1:  # if sizes are not equal
                    w, h = (min(math.ceil(w0 * r), self.imgsz), min(math.ceil(h0 * r), self.imgsz))
                    im = cv2.resize(im, (w, h), interpolation=cv2.INTER_LINEAR)
            elif not (h0 == w0 == self.imgsz):  # resize by stretching image to square imgsz
                im = cv2.resize(im, (self.imgsz, self.imgsz), interpolation=cv2.INTER_LINEAR)

            # Add to buffer if training with augmentations
            if self.augment:
                self.ims[i], self.im_hw0[i], self.im_hw[i] = im, (h0, w0), im.shape[:2]  # im, hw_original, hw_resized
                self.buffer.append(i)
                if 1 < len(self.buffer) >= self.max_buffer_length:  # prevent empty buffer
                    j = self.buffer.pop(0)
                    if self.cache != "ram":
                        self.ims[j], self.im_hw0[j], self.im_hw[j] = None, None, None

            return im, (h0, w0), im.shape[:2]

        return self.ims[i], self.im_hw0[i], self.im_hw[i]

    def generate_heatmap(self, img, anns):
        hm_h, hm_w = img.shape
        pre_hm = np.zeros((1, hm_h, hm_w), dtype=np.float32)
        pre_cts = []

        for ann in anns:
            class_id, x, y, w, h = map(float, ann.split(' '))
            # transfer yolo format 'xywh' to 'xyxy'
            bbox = [(x - w / 2) * hm_w, (y - h / 2) * hm_h, (x + w / 2) * hm_w, (y + h / 2) * hm_h]
            bbox_h, bbox_w = bbox[3] - bbox[1], bbox[2] - bbox[0]
            # calculate gaussian radius

            max_rad = 1
            if bbox_h > 0 and bbox_w > 0:
                radius = self.gaussian_radius((math.ceil(bbox_h), math.ceil(bbox_w)))
                radius = max(0, int(np.sqrt(bbox_h * bbox_w) / 2))
                max_rad = max(max_rad, radius)
                # print(radius, max_rad)
                ct = np.array([(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2], dtype=np.float32)
                ct_int = ct.astype(np.int32)
                pre_cts.append(ct)
                pre_hm[0] = self.draw_umich_gaussian(pre_hm[0], ct_int, radius, 255)

        return pre_hm[0], pre_cts

    def compare_histograms(self, img1, img2):
        if img1.shape != img2.shape:
            return 0
        hist1 = cv2.calcHist([img1], [0], None, [256], [0, 256])
        hist2 = cv2.calcHist([img2], [0], None, [256], [0, 256])

        cv2.normalize(hist1, hist1, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)
        cv2.normalize(hist2, hist2, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)
        similarity = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)
        return similarity

    def gaussian_radius(self, det_size, min_overlap=0.7):
        height, width = det_size

        a1 = 1
        b1 = (height + width)
        c1 = width * height * (1 - min_overlap) / (1 + min_overlap)
        sq1 = np.sqrt(b1 ** 2 - 4 * a1 * c1)
        r1 = (b1 + sq1) / 2

        a2 = 4
        b2 = 2 * (height + width)
        c2 = (1 - min_overlap) * width * height
        sq2 = np.sqrt(b2 ** 2 - 4 * a2 * c2)
        r2 = (b2 + sq2) / 2

        a3 = 4 * min_overlap
        b3 = -2 * min_overlap * (height + width)
        c3 = (min_overlap - 1) * width * height
        sq3 = np.sqrt(b3 ** 2 - 4 * a3 * c3)
        r3 = (b3 + sq3) / 2
        return min(r1, r2, r3)

    # @numba.jit(nopython=True, nogil=True)
    def gaussian2d(self, shape, sigma=1):
        m, n = [(ss - 1.) / 2. for ss in shape]
        y, x = np.ogrid[-m:m + 1, -n:n + 1]
        # y, x = np.arange(-m, m + 1).reshape(-1, 1), np.arange(-n, n + 1).reshape(1, -1)
        h = np.exp(-(x * x + y * y) / (2 * sigma * sigma))
        h[h < np.finfo(h.dtype).eps * h.max()] = 0
        return h

    # @numba.jit(nopython=True, nogil=True)
    def draw_umich_gaussian(self, heatmap, center, radius, k=1):
        # import pdb; pdb.set_trace()
        diameter = 2 * radius + 1
        gaussian = self.gaussian2d((diameter, diameter), sigma=diameter / 6)

        x, y = int(center[0]), int(center[1])

        height, width = heatmap.shape[0:2]

        left, right = min(x, radius), min(width - x, radius + 1)
        top, bottom = min(y, radius), min(height - y, radius + 1)
        # import pdb; pdb.set_trace()
        masked_heatmap = heatmap[y - top:y + bottom, x - left:x + right]
        masked_gaussian = gaussian[radius - top:radius + bottom, radius - left:radius + right]
        if min(masked_gaussian.shape) > 0 and min(masked_heatmap.shape) > 0:  # TODO debug
            np.maximum(masked_heatmap, masked_gaussian * k, out=masked_heatmap)
        return heatmap
        

ultralytics/data/build.py
from ultralytics.data.dataset import GroundingDataset, YOLODataset, YOLOMultiModalDataset, YOLOIRHMDataset

def build_yolo_dataset(cfg, img_path, batch, data, mode="train", rect=False, stride=32, multi_modal=False):
    """Build YOLO Dataset."""
    # dataset = YOLOMultiModalDataset if multi_modal else YOLODataset
    try:
        dataset = YOLOIRHMDataset
    except Exception as e:
        dataset = YOLOMultiModalDataset if multi_modal else YOLODataset

    return dataset(
        img_path=img_path,
        imgsz=cfg.imgsz,
        batch_size=batch,
        augment=mode == "train",  # augmentation
        hyp=cfg,  # TODO: probably add a get_hyps_from_cfg function
        rect=cfg.rect or rect,  # rectangular batches
        cache=cfg.cache or None,
        single_cls=cfg.single_cls or False,
        stride=int(stride),
        pad=0.0 if mode == "train" else 0.5,
        prefix=colorstr(f"{mode}: "),
        task=cfg.task,
        classes=cfg.classes,
        data=data,
        fraction=cfg.fraction if mode == "train" else 1.0,
    )
